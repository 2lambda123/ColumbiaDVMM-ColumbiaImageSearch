COMPOSE_HTTP_TIMEOUT=6000

# Should be set to $(git rev-parse --show-toplevel)
repo_path=/home/ubuntu/ColumbiaImageSearch
#repo_path=/media/data/Code/MEMEX/ColumbiaImageSearch
indocker_repo_path=/home/ubuntu/memex/ColumbiaImageSearch

# General verbose level (0-6)
verbose=4

# HT data from TF
input_nb_threads=4

# Extraction type
extr_conf_name=proc_sb_kinesis
extr_type=sbpycaffeimg
extr_nb_threads=4
extr_check_max_delay=600

# Kinesis input parameters
image_ingestion_type=kinesis
update_ingestion_type=hbase
images_stream=tf-images-sha1-test
create_stream=0
region_name=us-gov-west-1
aws_profile=kinesis
# Define shard_infos_filename to be some location that can be deleted from outside the docker
shard_infos_filename=/home/ubuntu/memex/ColumbiaImageSearch/kinesis_${images_stream}.json

# HBase settings (remote)
hbase_host=lb-emr-thrift-service-9e89c3c40cfc77c5.elb.us-gov-west-1.amazonaws.com

# In
in_table_sha1infos=mx_ht_images_details
in_image_buffer_column_family=img
in_image_buffer_column_name=img
in_image_info_column_family=data
in_image_url_column_name=location
in_extr_column_family=data

# Out
table_sha1infos=ht_images_infos_dev
table_updateinfos=ht_images_updates_dev
batch_update_size=2048
extr_column_family=data
image_info_column_family=data
image_buffer_column_family=img

# skip failed is still a bit experimental...
# Could be set to true once everything was run at least once,
# would never try to re-extract features from an corrupted/truncated image
#skip_failed=True




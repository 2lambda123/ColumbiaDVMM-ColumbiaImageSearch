COMPOSE_HTTP_TIMEOUT=600

# Should be set to $(git rev-parse --show-toplevel)
repo_path=/home/ubuntu/ColumbiaImageSearch
#repo_path=/media/data/Code/MEMEX/ColumbiaImageSearch
indocker_repo_path=/home/ubuntu/memex/ColumbiaImageSearch

# General verbose level (0-6)
verbose=4

# HT data from TF
input_nb_threads=4

# Extraction type
extr_conf_name=proc_sb_kinesis
extr_type=sbpycaffeimg
extr_nb_threads=4
extr_check_max_delay=600

# Kinesis input parameters
images_stream=tf-images-sha1-test
create_stream=0
#producer_type=kinesis
image_ingestion_type=kinesis
producer_prefix=LKP_
region_name=us-gov-west-1
aws_profile=kinesis
update_ingestion_type=hbase
# TODO: define shard_infos_filename to be some location that can be deleted from outside the docker


# HBase settings
# (remote)
# TODO: update that to load balancer URL
hbase_host=10.108.16.137
# In
in_table_sha1infos=mx_ht_images_details
in_image_buffer_column_family=img
in_image_buffer_column_name=img
in_image_info_column_family=data
in_image_url_column_name=location
in_extr_column_family=data

# Out
table_sha1infos=ht_images_infos_dev
table_updateinfos=ht_images_updates_dev
batch_update_size=2048
extr_column_family=data
image_info_column_family=data
image_buffer_column_family=img

# skip failed is still a bit experimental...
# Could be set to true once everything was run at least once,
# would never try to re-extract features from an corrupted/truncated image
#skip_failed=True




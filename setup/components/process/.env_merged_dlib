COMPOSE_HTTP_TIMEOUT=600

# Should be set to $(git rev-parse --show-toplevel)
repo_path=/home/ubuntu/ColumbiaImageSearch
#repo_path=/media/data/Code/MEMEX/ColumbiaImageSearch
indocker_repo_path=/home/ubuntu/memex/ColumbiaImageSearch

# General verbose level (0-6)
verbose=6

# HT data from TF
# Just needs to be something different from 'local', so 'file_input' would be false...
input_type=kafka
#input_type=kinesis

# Extraction type
extr_conf_name=proc_merged_dlib
extr_type=dlibface
extr_nb_threads=6
extr_check_max_delay=3600

# Kafka settings (remote)
image_ingestion_type=kafka
update_ingestion_type=hbase
kafka_servers=["kafka0.team-hg-memex.com:9093", "kafka1.team-hg-memex.com:9093", "kafka2.team-hg-memex.com:9093", "kafka3.team-hg-memex.com:9093", "kafka4.team-hg-memex.com:9093", "kafka5.team-hg-memex.com:9093", "kafka6.team-hg-memex.com:9093", "kafka7.team-hg-memex.com:9093", "kafka8.team-hg-memex.com:9093", "kafka9.team-hg-memex.com:9093"]
kafka_security={"security_protocol":"SSL","ssl_cafile":"/home/ubuntu/memex/ColumbiaImageSearch/data/keys/hg-kafka-ca-cert.pem","ssl_certfile":"/home/ubuntu/memex/ColumbiaImageSearch/data/keys/hg-kafka-client-cert.pem","ssl_keyfile":"/home/ubuntu/memex/ColumbiaImageSearch/data/keys/hg-kafka-client-key.pem","ssl_check_hostname":false}
images_topic=tf-images-sha1
images_consumer_group=tf-images-sha1-dlibface-extrchecker1

# This should be OK left undefined/empty, i.e. will fallback to only HBase.
# It is a more efficient to rely on Kafka for heavy workloads...
#update_ingestion_type=kafka
#updates_topic=tf-images-sha1-dlibface-updates
#updates_consumer_group=tf-images-sha1-dlibface-extrproc1

# Kinesis input parameters
#image_ingestion_type=kinesis
#update_ingestion_type=hbase
## Production stream
#images_stream=tf-images-sha1
#create_stream=0
#region_name=us-gov-west-1
#aws_profile=kinesis

# HBase settings
# (remote)
hbase_host=lb-emr-thrift-service-9e89c3c40cfc77c5.elb.us-gov-west-1.amazonaws.com

# In
in_table_sha1infos=mx_ht_images_details
in_image_buffer_column_family=img
in_image_buffer_column_name=img
in_image_info_column_family=data
in_image_url_column_name=location
in_extr_column_family=data

# Out
table_sha1infos=ht_images_infos_merged
table_updateinfos=ht_images_updates_merged
batch_update_size=2048
extr_column_family=data
image_info_column_family=data
image_buffer_column_family=img

# skip failed is still a bit experimental...
# Could be set to true once everything was run at least once,
# would never try to re-extract features from a corrupted/truncated image
#skip_failed=True




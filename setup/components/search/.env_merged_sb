COMPOSE_HTTP_TIMEOUT=600

# Should be set to $(git rev-parse --show-toplevel)
repo_path=/home/ubuntu/ColumbiaImageSearch
#repo_path=/media/data/Code/MEMEX/ColumbiaImageSearch
indocker_repo_path=/home/ubuntu/memex/ColumbiaImageSearch

# General verbose level (0-6)
verbose=4

# should be "local" for file input
input_type=kafka

# Extraction type
extr_type=sbpycaffeimg

# HBase settings
# (remote)
hbase_host=10.108.16.137
table_sha1infos=ht_images_infos_merged
table_updateinfos=ht_images_updates_merged
batch_update_size=2048
extr_column_family=data
image_info_column_family=data
image_buffer_column_family=img
image_buffer_column_name=img

# Searcher settings
search_conf_name=sbpycaffe_ht_release_lopqpca
model_type=lopq_pca
nb_train=2000000
nb_min_train=2000000
nb_train_pca=200000
nb_min_train_pca=200000
lopq_pcadims=256
lopq_V=4096
lopq_M=8
lopq_subq=256
file_input=false
storer=s3
# Need to create in REPOROOT/conf/aws_credentials/ a credentials file with the proper AWS infos,
# that file will be mounted as /home/ubuntu/.aws/credentials in the docker
aws_profile=cuimagesearch
aws_bucket_name=dig-cu-imagesearchindex
#aws_bucket_name=tellfinder-columbia
#aws_region=us-gov-west-1#aws_region=us-gov-west-1
#aws_prefix=index
reranking=true
# skip failed is still a bit experimental...
#searcher_skip_failed=True # Could be uncommented once every image have been reprocessed once

# API settings
port_host=80
endpoint=cuimgsearch
gunicorn_workers=16
gunicorn_timeout=1800
search_input=image